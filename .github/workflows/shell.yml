on:
  push:
  pull_request:

jobs:
  meta:
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/archlinux/archlinux:base-devel
    outputs:
      token: ${{ steps.kubeadm-token.outputs.token }}
      specs: ${{ steps.generate-specs.outputs.specs }}
    steps:
      - run: |
          sudo pacman -Syu --noconfirm kubeadm yq
      - id: kubeadm-token
        run: |
          echo token=$(kubeadm token generate) >> $GITHUB_OUTPUT

      - uses: actions/checkout@v4

      - name: Generate mac and ip
        id: generate-specs
        run: |
          i=86
          j=10

          echo '---' > specs.yml

          for name in kmaster kworker; do
            mac=$(printf '52:54:00:12:34:%02x\n' $i)
            ip="192.168.100."$j
            i=$((i+1))
            j=$((j+1))
            echo "$name mac: $mac ip: $ip"

            echo "- name: $name" >> specs.yml
            echo "  hostname: $name-vm" >> specs.yml
            echo "  mac: $mac" >> specs.yml
            echo "  ip: $ip" >> specs.yml

            #if [[ $name == "kmaster" ]]; then
            #  echo "  cni: cilium" >> specs.yml

            #  echo "- name: $name" >> specs.yml
            #  echo "  hostname: $name-vm" >> specs.yml
            #  echo "  mac: $mac" >> specs.yml
            #  echo "  ip: $ip" >> specs.yml
            #  echo "  cni: flannel" >> specs.yml
            #fi

            cat > systemd-network/host/10-k8s-bridge.network.d/$name.conf << EOF
          [DHCPServerStaticLease]
          MACAddress=$mac
          Address=$ip
          EOF
          done

          cat specs.yml | yq > specs.json
          cat specs.json >> $GITHUB_STEP_SUMMARY

          echo specs=$(cat specs.json) >> $GITHUB_OUTPUT

      - uses: actions/upload-artifact@v5
        with:
          name: systemd-network-host
          path: systemd-network/host

  cloud-init:
    needs: [meta]
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include: ${{ fromJson(needs.meta.outputs.specs) }}
      fail-fast: false
    env: ${{ matrix }}
    steps:
      - name: Update and install dependencies
        run: |
          sudo apt-get update
          sudo apt-get -y install qemu-system-x86 qemu-utils cloud-utils sshpass
          # pacman -Syu --noconfirm
          # pacman -S --noconfirm git qemu-system-x86

      - uses: actions/checkout@v4
      - uses: actions/download-artifact@v6
        with:
          name: systemd-network-host
          path: systemd-network-host

      - name: Config ssh and networking
        run: |
          sudo -u $USER -H sh -c 'mkdir -p ~/.ssh && chmod 700 ~/.ssh'
          sudo -u $USER -H sh -c 'ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa'

          sudo tee /etc/ssh/ssh_config << EOF
          Host *
              StrictHostKeyChecking accept-new
          EOF

          sudo cp -vr systemd-network-host/* /etc/systemd/network/
          journalctl -f &
          sudo systemctl restart systemd-networkd

          sleep 5

          # a docker rule was blocking forwarding, allow it here
          sudo iptables -I FORWARD -i br-k8s -o eth0 -j ACCEPT
          sudo iptables -I FORWARD -i eth0 -o br-k8s -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT

          networkctl status --all
          ip route

          killall journalctl || true

      - name: Setup KVM
        run: |
            echo 'KERNEL=="kvm", GROUP="kvm", MODE="0666", OPTIONS+="static_node=kvm"' | sudo tee /etc/udev/rules.d/99-kvm4all.rules
            sudo udevadm control --reload-rules
            sudo udevadm trigger --name-match=kvm

            stat /dev/kvm

      - name: Prepare cloud-init seed
        run: |
          key=$(sudo -u $USER -H sh -c 'cat ~/.ssh/id_rsa.pub')

          echo local-hostname: ${{ matrix.name }}-vm >> ./meta-data.yml
          echo "ssh_key: $key" >> ./meta-data.yml

          cloud-localds -v --disk-format qcow2 seed.qcow2 ./cloud-init/user-data.yml ./meta-data.yml

      - name: Arch Linux cloud image
        run: |
          wget --progress=bar:force https://geo.mirror.pkgbuild.com/images/latest/Arch-Linux-x86_64-cloudimg.qcow2

          qemu-img resize Arch-Linux-*.qcow2 +10G

          ln -sf Arch-Linux-*.qcow2 rootfs.qcow2

      - name: Boot
        run: |
          export HOSTNAME=${{ matrix.name }}-vm
          export MAC=${{ matrix.mac }}

          # --slice=machine.slice

          systemd-run --user -d --unit=$HOSTNAME \
          qemu-system-x86_64 -m 2G -enable-kvm -cpu host -smp 2 \
            -nic tap,ifname=tap-$HOSTNAME,script=no,downscript=no,vhost=off,model=virtio-net-pci,mac=$MAC \
            -drive file=rootfs.qcow2,if=virtio \
            -drive file=seed.qcow2,if=virtio \
            -display none \
            -serial stdio

          journalctl --user -f --no-hostname --all -u $HOSTNAME &

          echo ::group::Waiting for VM to boot
          timeout -v 5m bash -c "until ip neigh show | tee /dev/stderr | grep $MAC\ REACHABLE; do sleep 2; done"
          echo ::endgroup::

          export IP=$(ip neigh show | grep "$MAC REACHABLE" | awk '{print $1}')
          echo $IP $HOSTNAME | sudo tee -a /etc/hosts

          echo ::group::Waiting for SSH to be available
          timeout -v 5m bash -c "until nc -zv $HOSTNAME 22; do sleep 2; done"
          echo ::endgroup::

          ssh-keyscan -v $HOSTNAME

          sudo tee -a /etc/ssh/ssh_config << EOF
          Host $HOSTNAME
              User arch
              StrictHostKeyChecking accept-new
          Host vm
              Hostname $HOSTNAME
              User arch
              StrictHostKeyChecking accept-new
          EOF

          ssh vm true

          echo ::group::Waiting for systemd to be running
          timeout -v 5m ssh vm bash -e << EOF
          until systemctl is-system-running; do
            systemctl list-jobs
            systemctl --failed
            sleep 5
          done
          EOF
          echo ::endgroup::

          ssh vm hostnamectl
          ssh vm 'sudo usermod --password $(echo arch | openssl passwd -1 -stdin) arch'
          sudo -u $USER -H sh -c 'cp ~/.ssh/id_rsa .'

          kill $(jobs -p) || true

      - name: Resize root filesystem
        run: |
          ssh vm sudo mount -t tmpfs tmpfs /var/cache/pacman/pkg

          ssh vm df -h
          ssh vm sudo pacman -Sy --noconfirm
          ssh vm sudo pacman -S --noconfirm parted
          yes | ssh vm sudo parted /dev/vda ---pretend-input-tty  resizepart 3 100% Yes
          ssh vm sudo btrfs filesystem resize max /
          ssh vm df -h

      - name: Install CRI-O
        run: |
          yes | ssh vm sudo pacman -S iptables-nft
          ssh vm sudo pacman -S --noconfirm cri-o --assume-installed libkrun

          ssh vm sudo systemctl enable --now crio
          ssh vm sudo crio status info

      - name: Config kubeadm init
        if: matrix.name == 'kmaster'
        run: |
          ssh vm sudo pacman -S --noconfirm kubernetes-tools kubelet cilium-cli
          ssh vm sudo systemctl enable --now kubelet.service
          ssh vm sudo kubeadm -v=5 init \
            --cri-socket='unix:///run/crio/crio.sock' \
            --pod-network-cidr=10.244.0.0/16 \
            --token=${{ needs.meta.outputs.token }}

      - name: Config kubeadm join
        if: matrix.name != 'kmaster'
        run: |
          ssh vm sudo pacman -S --noconfirm kubernetes-tools kubelet
          ssh vm sudo systemctl enable --now kubelet.service

          ssh vm sudo kubeadm config images pull

          ssh vm sudo tee /etc/systemd/system/kubeadm-join.service << EOF
          [Unit]
          Description=Kubeadm Join Service
          After=network-online.target
          Wants=network-online.target
          After=kubelet.service
          Requires=crio.service

          [Service]
          Type=oneshot
          ExecStart=/usr/bin/kubeadm -v=6 join kmaster-vm:6443 \
            --cri-socket='unix:///run/crio/crio.sock' \
            --token=${{ needs.meta.outputs.token }} \
            --discovery-token-unsafe-skip-ca-verification
          Restart=on-failure

          [Install]
          WantedBy=multi-user.target
          EOF

          ssh vm sudo systemctl daemon-reload
          ssh vm sudo systemctl enable kubeadm-join.service

      - name: Set up kubeconfig
        if: matrix.name == 'kmaster'
        run: |
          ssh kmaster-vm 'mkdir -p $HOME/.kube'
          ssh kmaster-vm 'sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config'
          ssh kmaster-vm 'sudo chown $USER:$USER $HOME/.kube/config'

      - name: Install Cilium CNI
        if: false # matrix.name == 'kmaster'
        run: |
          ssh vm sudo env KUBECONFIG=/etc/kubernetes/admin.conf cilium-cli install
          ssh vm sudo env KUBECONFIG=/etc/kubernetes/admin.conf cilium-cli status --wait

      - name: Install Flannel CNI
        if: matrix.name == 'kmaster' # s&& matrix.cni == 'flannel'
        run: |
          timeout -v 5m ssh kmaster-vm bash -ex << EOF
          # journalctl -f -u kubelet &
          kubectl get events --watch --all-namespaces &

          kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

          sh -c 'kubectl get pods -n kube-flannel --watch -o name | xargs -n1 -P0 kubectl -n kube-flannel logs' &

          kubectl get nodes -o wide
          kubectl wait --for=condition=Ready node/kmaster-vm
          kubectl get nodes -o wide

          kubectl get pods --all-namespaces -o wide
          kubectl wait --for=condition=Ready --all pods --all-namespaces
          kubectl get pods --all-namespaces -o wide

          kill $(jobs -p) || true
          killall kubectl || true
          EOF

      - name: Print configuration
        if: matrix.name == 'kmaster'
        run: |
          echo 'kubeadm config:' >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          ssh vm kubectl -n kube-system get configmaps kubeadm-config --output yaml | tee -a $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: Shutdown control plane
        if: matrix.name == 'kmaster'
        run: |
          ssh vm kubectl cordon kmaster-vm
          ssh vm kubectl drain kmaster-vm --ignore-daemonsets --delete-emptydir-data

          ssh vm journalctl -f &

          ssh vm sudo mount -t tmpfs tmpfs /etc/kubernetes/manifests
          sleep 10
          ssh vm sudo systemctl stop kubelet

          set +e

          ssh vm sudo bash -x 'crictl pods'
          ssh vm sudo bash -x 'crictl pods -q | xargs -r crictl stopp'
          sleep 10
          ssh vm sudo systemctl stop crio

      - name: Poweroff
        run: |
          ssh vm sudo systemctl status

          ssh vm sudo systemctl poweroff || true

          journalctl --user -f --no-hostname --all -u $HOSTNAME &
          tail --pid=$(pidof qemu-system-x86_64) -f /dev/null
          kill $(jobs -p) || true

      - name: Dump VM journal
        if: always()
        run: |
          killall qemu-system-x86_64 || true
          sudo modprobe nbd
          sudo qemu-nbd --read-only --connect=/dev/nbd0 rootfs.qcow2
          sudo fdisk -l /dev/nbd0
          sudo mount -oro /dev/nbd0p3 /mnt
          sudo ls -l /mnt/var/log/journal

          echo ::group::
          sudo journalctl --directory=/mnt/var/log/journal | tee vm-journal.log
          echo ::endgroup::

      - uses: actions/upload-artifact@v5
        if: always()
        with:
          name: ${{ matrix.name }}-vm-${{ matrix.cni && format('{0}-image', matrix.cni) || 'image' }}
          path: 'rootfs.qcow2'

  run-cluster:
    needs: [meta, cloud-init]
    runs-on: ubuntu-latest
    #strategy:
    #  matrix:
    #    cni: [cilium, flannel]
    steps:
      - name: Update and install dependencies
        run: |
          sudo apt-get update
          sudo apt-get -y install qemu-system-x86 qemu-utils cloud-utils sshpass
          # pacman -Syu --noconfirm
          # pacman -S --noconfirm git qemu-system-x86

      - uses: actions/checkout@v4

      - name: Config ssh and networking
        run: |
          sudo -u $USER -H sh -c 'mkdir -p ~/.ssh && chmod 700 ~/.ssh'
          sudo -u $USER -H sh -c 'ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa'

          sudo tee -a /etc/ssh/ssh_config << EOF
          Host *
              StrictHostKeyChecking accept-new
          EOF

          sudo cp -vr systemd-network/host/* /etc/systemd/network/
          journalctl -f &
          sudo systemctl restart systemd-networkd

          sleep 5

          # a docker rule was blocking forwarding, allow it here
          sudo iptables -I FORWARD -i br-k8s -o eth0 -j ACCEPT
          sudo iptables -I FORWARD -i eth0 -o br-k8s -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT

          networkctl status --all
          ip route

          killall journalctl || true

      - name: Setup KVM
        run: |
          echo 'KERNEL=="kvm", GROUP="kvm", MODE="0666", OPTIONS+="static_node=kvm"' | sudo tee /etc/udev/rules.d/99-kvm4all.rules
          sudo udevadm control --reload-rules
          sudo udevadm trigger --name-match=kvm

          stat /dev/kvm

      - uses: actions/download-artifact@v6

      - name: Boot kmaster VM
        run: |
          export HOSTNAME=kmaster-vm
          export MAC=52:54:00:12:34:56

          systemd-run --user -d --unit=$HOSTNAME \
          qemu-system-x86_64 -m 2G -enable-kvm -cpu host -smp 2 \
            -nic tap,ifname=tap-$HOSTNAME,script=no,downscript=no,vhost=off,model=virtio-net-pci,mac=$MAC \
            -drive file=$HOSTNAME-image/rootfs.qcow2,if=virtio \
            -display none \
            -serial stdio

          journalctl --user -f --no-hostname --all -u $HOSTNAME &

          until ip neigh show | grep "$MAC REACHABLE"; do sleep 5; done

          export IP=$(ip neigh show | grep "$MAC REACHABLE" | awk '{print $1}')
          echo $IP $HOSTNAME | sudo tee -a /etc/hosts

          until ssh-keyscan -v $HOSTNAME; do sleep 5; done

          sudo tee -a /etc/ssh/ssh_config << EOF
          Host $HOSTNAME
              User arch
              StrictHostKeyChecking accept-new
          EOF

          sshpass -p 'arch' ssh-copy-id -o StrictHostKeyChecking=accept-new $HOSTNAME

          ssh $HOSTNAME journalctl -f &

          ssh $HOSTNAME bash -e <<EOF
          until kubectl wait --for=condition=Ready node/kmaster-vm --timeout=5s;
          do true; done
          EOF

          kill $(jobs -p) || true

      - name: Boot kworker VM
        run: |
          export HOSTNAME=kworker-vm
          export MAC=52:54:00:12:34:57

          systemd-run --user -d --unit=$HOSTNAME \
          qemu-system-x86_64 -m 2G -enable-kvm -cpu host -smp 2 \
            -nic tap,ifname=tap-$HOSTNAME,script=no,downscript=no,vhost=off,model=virtio-net-pci,mac=$MAC \
            -drive file=$HOSTNAME-image/rootfs.qcow2,if=virtio \
            -display none \
            -serial stdio

          journalctl --user -f --no-hostname --all -u $HOSTNAME &

          until ip neigh show | grep "$MAC REACHABLE"; do sleep 5; done

          export IP=$(ip neigh show | grep "$MAC REACHABLE" | awk '{print $1}')
          echo $IP $HOSTNAME | sudo tee -a /etc/hosts

          until ssh-keyscan -v $HOSTNAME; do sleep 5; done

          sudo tee -a /etc/ssh/ssh_config << EOF
          Host $HOSTNAME
              User arch
              StrictHostKeyChecking accept-new
          EOF

          sshpass -p 'arch' ssh-copy-id -o StrictHostKeyChecking=accept-new $HOSTNAME

          grep -e '-vm' /etc/hosts | ssh $HOSTNAME sudo tee -a /etc/hosts
          grep -e '-vm' /etc/hosts | ssh kmaster-vm sudo tee -a /etc/hosts

          ssh $HOSTNAME journalctl -f &

          timeout -v 5m ssh $HOSTNAME bash -e << EOF
          until systemctl is-system-running; do
            systemctl list-jobs
            systemctl --failed
            sleep 5
          done
          EOF

          kill $(jobs -p) || true

      - if: ${{ failure() }}
        run: |
          ssh kworker-vm journalctl -u kubeadm-join.service
          ssh kworker-vm journalctl -u kubelet.service
          ssh kworker-vm journalctl -u crio.service

      - name: Verify cluster
        run: |
          timeout -v 5m ssh kmaster-vm bash -e << EOF

          until kubectl wait --for=condition=Ready node/kworker-vm; do kubectl get nodes -o wide; done

          kubectl get nodes -o wide

          until kubectl wait --for=condition=Ready --all pods --all-namespaces; do
            kubectl get pods --all-namespaces -o wide
          done

          kubectl get pods --all-namespaces -o wide

          EOF
